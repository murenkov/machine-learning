{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6413596a-2a2e-45c8-93b0-cd872b27b85c",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4\n",
    "\n",
    "Лабораторная работа построена на фреймворках:\n",
    "\n",
    "* [Keras](https://keras.io/guides) для построения нейронных сетей;\n",
    "* [matplotlib](https://matplotlib.org/stable/tutorials/index.html) для работы с графиками;\n",
    "* [numpy](https://numpy.org/doc/stable/) для работы с матрицами и векторами;\n",
    "* [sklearn](https://scikit-learn.org/stable/user_guide.html) для тонкой настройки параметров с помощью кросс-валидации.\n",
    "\n",
    "Для начала подключим необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e958b4e-1298-4fcf-815c-6fc2a59019c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import datasets, model_selection\n",
    "from collections import namedtuple\n",
    "import timeit\n",
    "\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, *args):\n",
    "        assert len(args) == 2 or len(args) == 4, 'x_train, x_test, y_train, y_test or (x_train, y_train), (x_test, y_test)'\n",
    "        if len(args) == 2:\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test) = args\n",
    "        else:\n",
    "            self.x_train, self.x_test, self.y_train, self.y_test = args\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return (self.x_train, self.y_train)\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return (self.x_test, self.y_test)\n",
    "\n",
    "print(\"tensorflow =\", tf.__version__, \"keras =\", keras.__version__)\n",
    "if tuple(map(int, tf.__version__.split('.'))) < (2, 5, 0):\n",
    "    tf.autograph.set_verbosity(0)\n",
    "    import logging\n",
    "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7542ff6-c5af-4805-bea8-42d485f07716",
   "metadata": {},
   "source": [
    "## Часть №1. Разработка компонентов нейронной сети\n",
    "\n",
    "Первая часть посвящена разработке слоёв нейронной сети.\n",
    "Заранее будет определён класс `MyModel` для работы с моделью нейронной сети.\n",
    "Целью этого класса является:\n",
    "\n",
    "* Объединение всех слоёв в единую структуру.\n",
    "* Организация обратного распространения ошибки.\n",
    "* Отслеживание метрики штрафа и точности.\n",
    "* Использование оптимизатора для уточнения весов сети (в данной работе будет использоваться градиентный спуск).\n",
    "* Клонирование модели для поиска гипер параметров сети методом кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae10d1-6c42-4502-8285-eac4316c1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=0)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def clone(self):\n",
    "        clone_ = lambda x: x.from_config(x.get_config())\n",
    "        \n",
    "        model = keras.models.clone_model(self)\n",
    "        model.set_weights(self.get_weights())\n",
    "        metrics = [clone_(x) for x in self.compiled_metrics._metrics]\n",
    "        model.compile(optimizer=clone_(self.optimizer), loss=clone_(self.loss), metrics=metrics)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746274fd-0af0-422c-9623-6a6bb4cc18e6",
   "metadata": {},
   "source": [
    "### Линейный слой\n",
    "\n",
    "Разработайте модель линейного слоя.\n",
    "Перемножение матриц осуществляется с помощью функции `tf.matmul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111afb7-21e4-4fd3-97b9-b85193247ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, seed, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        self.units_ = units\n",
    "        self.seed_ = seed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = keras.initializers.RandomNormal(???, seed=self.seed_)\n",
    "        self.w = self.add_weight(shape=(???), initializer=w_init, trainable=True)\n",
    "        self.b = self.add_weight(shape=(???), initializer=keras.initializers.Zeros(), trainable=True)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        return ??? tf.matmul\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {**super(MyLayer, self).get_config(), 'units': self.units_, 'seed': self.seed_}\n",
    "\n",
    "# Тестирование\n",
    "\n",
    "mylayer_test = MyLayer(20, seed=56)\n",
    "\n",
    "assert mylayer_test(keras.Input(shape=(100,))).shape[1] == 20, 'Не поддержан метод build'\n",
    "assert mylayer_test(tf.ones([3000, 100])).shape == (3000, 20), 'Не сохраняются веса после вызова метода build'\n",
    "\n",
    "w = mylayer_test.get_weights()\n",
    "assert len(w) == 2, 'Веса деляется на коэффициенты и смещения'\n",
    "assert w[0].shape == (100, 20), 'Неправильное количество коэффициентов'\n",
    "assert w[1].shape == (20,), 'Неправильно количество смещений'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d7aa9-7b98-441b-bf78-4932e2abacdd",
   "metadata": {},
   "source": [
    "### Градиентный спуск\n",
    "\n",
    "В качестве оптимизатора градиентов будем использовать градиентный спуск.\n",
    "Реализуйте его в следующем классе.\n",
    "В качестве возращаемого значения используйте конструкцию `var.assign(...)`, в которую дожна помещаться формула градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f0e18-caf6-496a-b8b6-98e31af7bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradDescent(keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, name=\"GradDescent\", **kwargs):\n",
    "        super(GradDescent, self).__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"learning_rate\", learning_rate))\n",
    "        self._set_hyper(\"decay\", self._initial_decay)\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        lr = self._decayed_lr(var.dtype.base_dtype)\n",
    "        return var.assign(???)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {**super(GradDescent, self).get_config(), 'learning_rate': self._serialize_hyperparameter(\"learning_rate\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca17fe7-2cff-4edf-ab79-0dbf7731a138",
   "metadata": {},
   "source": [
    "### Подготовка датасета\n",
    "\n",
    "Разрабатывать нейросеть будем на распространённом датасете `mnist`.\n",
    "Загрузка обучающей и тестовой выборок осуществляется с помощью метода `load_data` из библиотеки `keras.datasets.mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6567c6c-8313-4982-b450-ae6fc48b25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = MyDataset(*keras.datasets.mnist.load_data())\n",
    "\n",
    "print(f\"Размерности: x_train={d1.x_train.shape}:{d1.x_train.dtype}, y_train={d1.y_train.shape}:{d1.y_train.dtype}, x_test={d1.x_test.shape}, y_test={d1.y_test.shape}\")\n",
    "\n",
    "d1.x_train = d1.x_train.reshape(-1, np.prod(d1.x_train.shape[1:])) / 255.0\n",
    "d1.x_test = d1.x_test.reshape(-1, np.prod(d1.x_test.shape[1:])) / 255.0\n",
    "\n",
    "print(f\"Размерности после преобразования: x_train={d1.x_train.shape}, y_train={set(d1.y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c648879-8815-4f4b-adad-c0cdb2045c03",
   "metadata": {},
   "source": [
    "Впросы:\n",
    "\n",
    "1. Каков размер обучающей выборки?\n",
    "1. Какую оригинальную размерность имеет каждый обучающий вектор?\n",
    "1. Каков размер тестовой выборки?\n",
    "1. Какую размерность и значения имеет целевой вектор?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff5d0b-9e07-4189-b323-4a747a6792a6",
   "metadata": {},
   "source": [
    "Ответы\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ec776-8810-4286-83f8-7f25f7c3ec55",
   "metadata": {},
   "source": [
    "### Определение архитектуры нейронной сети\n",
    "\n",
    "Для определения архитектуры нейронной сети будем использовать объектный подход, который заключается в следующем.\n",
    "Создаём абстрактные данные (входной слой) нужной размерности, но без уточнения количества векторов.\n",
    "Рекурсивно вызываем все слои, входом которых будет выход с предыдущего слоя.\n",
    "После этого создаём модель, входном которой являются абстрактные данные, созданные вначале, а выходом является выход самого последнего слоя.\n",
    "Далее модель компилируется с помощью задания оптимизатора градиентного спуска (optimizer), функции потерь (loss) и функций метрик (metrics).\n",
    "Полученную модель будем использовать как прототип, чтобы иметь возможность сбрасывать до первоначального состояния значения всех весов и гипер параметров нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae2a06-b079-476a-9843-1c2973be2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(d1.x_train.shape[-1],), name=\"digits\")\n",
    "x = MyLayer(len(set(d1.y_train)), seed=42, name=\"layer_1\")(inputs)\n",
    "outputs = keras.layers.Activation(keras.activations.softmax, name=\"softmax_1\")(x)\n",
    "\n",
    "mproto = MyModel(inputs=inputs, outputs=outputs)\n",
    "mproto.compile(optimizer=GradDescent(0.1),\n",
    "               loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "               metrics=[keras.metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "mproto.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0d6cd-df91-4a34-b3ea-135c16e6aaeb",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Какова размерность входных данных нейронной сети?\n",
    "1. Сколько всего параметров у нейронной сети?\n",
    "1. Сколько слоёв имеет нейронная сеть?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c6f0b-6533-45db-a186-4aa27438ae06",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87632863-7055-4345-b11f-0b6de1ec2740",
   "metadata": {},
   "source": [
    "### Запуск полученной модели\n",
    "\n",
    "Обучение нейронной сети осуществляется с помощью метода `fit`.\n",
    "С помощью параметра `verbose` можно регулировать степень логирования процесса обучения.\n",
    "В примере будем использовать параметр `validation_data`, который используется для запуска модели на переданных данных для получения метрик качества.\n",
    "Параметр подразумевает, что будуте переданы валидационные данные, однако мы будем передавать туда наши тестовые векторы.\n",
    "Мы можем так делать, потому что в данной модели валидационные данные **не влияют на обучение**.\n",
    "Чтобы разобраться в отличии валидационных данных от тестовых, советуем ознакомиться с [этой дискуссией](https://github.com/keras-team/keras/issues/1753).\n",
    "\n",
    "Постройте графики зависимости функции штрафа от количества эпох и точность классификации от количества эпох для обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f9802-af6c-4cf5-aea8-adc954ad3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = mproto.clone()\n",
    "history = model.fit(*d1.train, batch_size=64, epochs=10, verbose=1, validation_data=d1.test)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(2*6, 4))\n",
    "ax0.plot(history.history['loss'], label='train')\n",
    "ax0.plot(history.history['val_loss'], label='test')\n",
    "ax0.set(xlabel='Номер эпохи', ylabel='Значение штрафа', title='Loss')\n",
    "ax0.legend()\n",
    "ax1.plot(history.history['accuracy'], label='train')\n",
    "ax1.plot(history.history['val_accuracy'], label='test')\n",
    "ax1.set(xlabel='Номер эпохи', ylabel='Точность', title='Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "print(dict(zip(model.metrics_names, model.evaluate(*d1.test, verbose=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae968ae5-3dda-45b5-b8cf-e05118c1a92c",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Сколько итераций было за эпоху?\n",
    "1. Сколько использовалось векторов в одной итерации?\n",
    "1. Какой точности удалось достичь?\n",
    "1. Опишите поведение функции потерь и точности для обучающей и тестовой последовательностям. Насколько ни похожи?\n",
    "1. Есть ли переобучение модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f50d50-21d6-4973-94d0-3a9553d602d1",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26787d-d460-4aa9-b90c-5822b109876a",
   "metadata": {},
   "source": [
    "Покажите, что получается на выходе нейронной сети, построив графики распределения ответов на последнем слое.\n",
    "Также покажите на хит-мапе корреляцию ответов нейронной сети с реальными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcc48b-e42d-4725-a87b-81ea2b214f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(d1.x_test)\n",
    "for idx, pp in enumerate(p[:4]):\n",
    "    plt.bar(np.arange(0, 10), pp, label=d1.y_test[idx])\n",
    "\n",
    "plt.xlabel('Значение')\n",
    "plt.ylabel('Вероятность')\n",
    "plt.title('Предсказание')\n",
    "plt.xticks(np.arange(0, 10))\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(tf.math.confusion_matrix(d1.y_test, p.argmax(axis=1)), cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.xticks(list(set(d1.y_test)))\n",
    "plt.yticks(list(set(d1.y_test)))\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f078c1-4aa1-4e5d-b2a0-fae202f681e8",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Что является выходом нейронной сети?\n",
    "1. Какие значения принимает `y_train`?\n",
    "1. Как нейронная сеть сравнивает свой выход с `y_train`?\n",
    "1. Какая цифра распознаётся лучше всех, а какая хуже всех?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2f210-8065-4e46-a96a-40584f9c818d",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5dac6-917f-4834-9771-3de55493f1d9",
   "metadata": {},
   "source": [
    "## Часть №2. Поиск оптимальных гиперпараметров нейронной сети\n",
    "\n",
    "В этой части рассмотрим как происходит настройка гиперпараметров нейронной сети.\n",
    "Гиперпараметр является лучшим, если при нём нейросеть показывает наилучшие характеристике на _тестовом_ датасете.\n",
    "Тестирование во время обучения осуществляется с помощью механизма кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48a8be-4f3b-48cc-b0e2-01b9883790de",
   "metadata": {},
   "source": [
    "### Загрузка датасета\n",
    "\n",
    "Для демонстрации поиска параметров нейронной сети будем использовать искусственный датасет.\n",
    "Получить его можно функцией `sklearn.datasets.make_classification`.\n",
    "Параметры функции можно посмотреть в документации.\n",
    "Нас пока будут интересовать параметры:\n",
    "\n",
    "* `n_samples` - количество векторов обучения;\n",
    "* `n_classes` - количество классов для классификации;\n",
    "* `random_state` - для повторяемости генерации датасета;\n",
    "* остальные параметры нужны для балансировки датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e17d2f-64a6-4c75-a838-cf23c698c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sk.datasets.make_moons(n_samples=1000, random_state=1)\n",
    "d2 = MyDataset(*sk.model_selection.train_test_split(x, y, test_size=0.2))\n",
    "\n",
    "print(f\"x_train={d2.x_train.shape}, y_train={d2.y_train.shape}:{set(d2.y_train)}, x_test={d2.x_test.shape}\")\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for x, y, ax in [(d2.x_train, d2.y_train, ax0), (d2.x_test, d2.y_test, ax1)]:\n",
    "    ax.scatter(x[:, 0], x[:, 1], c=y)\n",
    "    ax.set(xlabel='X0', ylabel='X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa648f-b868-4ae0-bd62-c62ff6387af1",
   "metadata": {},
   "source": [
    "### Поиск оптимальных параметров сети\n",
    "\n",
    "Создание модели помещено в функцию, чтобы можно было \"клонировать\" нейронную сеть с изменёнными параметрами.\n",
    "Параметр `batch_size` не входит в саму сеть, он будет подставляться уже при запуске обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825c613-a496-4e6c-9a21-2cd017eb60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(units=50, activation='relu', lr=0.1):\n",
    "    inputs = keras.Input(shape=(???))\n",
    "    x = MyLayer(???, seed=49)(???)\n",
    "    x = keras.layers.Activation(???)(x)\n",
    "    x = MyLayer(???, seed=75)(???)\n",
    "    outputs = keras.layers.Activation(keras.activations.sigmoid)(x)\n",
    "\n",
    "    model = MyModel(inputs, outputs)\n",
    "    model.compile(GradDescent(lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "params = {'lr': np.logspace(-2, 1.3, 7),\n",
    "          'units': [2, 5, 10, 50, 100, 500, 1000],\n",
    "          'batch_size': [1, 16, 64, 128, 1024],\n",
    "          'activation': ['relu', 'tanh', 'softmax', 'sigmoid']\n",
    "         }\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2778a3-7ac4-471d-b999-a2caf5cd384d",
   "metadata": {},
   "source": [
    "#### Исследование коэффициента скорости поиска\n",
    "\n",
    "Переберите все параметры коэффициента скорости градиентного спуска `lr` и постройте графики:\n",
    "* зависимость функции штрафа от номера эпохи,\n",
    "* зависимость точности классификации от номера эпохи для обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65298a6c-1913-4f73-a94e-0305668e5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(3*6, 4))\n",
    "for p in params['lr']:\n",
    "    model = create_model(lr=p)\n",
    "    history = model.fit(*d2.train, validation_data=d2.test, epochs=100, verbose=0)\n",
    "    ax0.plot(???, label=p)\n",
    "    ax0.set(xlabel='Номер эпохи', ylabel='Функция штрафа', title='Loss', ylim=[0, 2.5])\n",
    "    ax0.legend()\n",
    "    ax1.plot(???, label=p)\n",
    "    ax1.set(xlabel='Номер эпохи', ylabel='Точность', title='Accuracy')\n",
    "    ax1.legend()\n",
    "    ax2.plot(???, label=p)\n",
    "    ax2.set(xlabel='Номер эпохи', ylabel='Точность', title='Test Accuracy')\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d52f71-0da6-468f-8854-cfbd698691a5",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Что было бы, если бы в примере было гораздо больше эпох (можно поэкспериментировать с количеством эпох)?\n",
    "1. При всех ли коэффициентах `lr` достигается 100% точность (нулевая функция потерь)?\n",
    "1. Сформулируйте правило: чем больше значение `lr`, тем скорость обучения выше/ниже?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b0525-239e-4a5e-8b47-d9a425bf90ed",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcf892-16ef-4108-b09b-cac2ca996912",
   "metadata": {},
   "source": [
    "#### Исследование размера скрытого слоя\n",
    "\n",
    "Размером скрытого слоя будем называть количество содержащихся в нём нейронов.\n",
    "Изучим, как количество нейронов в скрытом слое влияет на точность и скорость обучения.\n",
    "Постройте графики зависимостей:\n",
    "\n",
    "* функции потерь от номера эпохи;\n",
    "* точность классификации от номера эпохи для обучающей и тестовой выборок;\n",
    "* время обучения модели от количества нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5ea67-d153-4a25-b37f-5d861002ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(3*6, 4))\n",
    "layer_timer = {}\n",
    "for p in params['units']:\n",
    "    model = create_model(units=p, lr=1.5)\n",
    "    t = timeit.default_timer()\n",
    "    history = model.fit(*d2.train, validation_data=d2.test, epochs=100, verbose=0)\n",
    "    layer_timer[p] = timeit.default_timer() - t\n",
    "    ax0.plot(???)\n",
    "    ???\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Количество нейронов')\n",
    "plt.ylabel('Время, секунды')\n",
    "plt.title('Производительность')\n",
    "plt.plot(layer_timer.keys(), layer_timer.values()) # python3.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e8d6b-4bb2-4fd2-938d-6bf5e9353463",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Как связаны скорость обучения и размер скрытого слоя?\n",
    "1. Как связаны время обучения (секунды, потраченные на обучение) и размер скрытого слоя?\n",
    "1. Что наблюдается при очень малом количестве нейронов в скрытом слое?\n",
    "1. Что наблюдается при очень большом количестве нейронов в скрытом слое?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918e849-648f-4cd3-b22c-14cc46b24da5",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3cb72c-3df9-4113-b4b2-b0ee76b1269c",
   "metadata": {},
   "source": [
    "#### Исследование размера батча\n",
    "\n",
    "Размер батча определяется в методе `fit` настроенной модели параметром `batch_size`.\n",
    "Постройте графики зависимостей:\n",
    "\n",
    "* функции потерь от номера эпохи;\n",
    "* точность классификации от номера эпохи для обучающей и тестовой выборок;\n",
    "* время обучения модели от количества нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33ce34-b4a9-4c59-93dc-acfe684ca002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(3*6, 4))\n",
    "batch_timer = {}\n",
    "for p in params['batch_size']:\n",
    "    model = create_model(units=50, lr=1.5)\n",
    "    t = timeit.default_timer()\n",
    "    history = model.fit(*d2.train, validation_data=d2.test, epochs=100, batch_size=p, verbose=0)\n",
    "    ???\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Размер батча')\n",
    "plt.ylabel('Время, секунды')\n",
    "plt.title('Производительность')\n",
    "plt.semilogx(batch_timer.keys(), batch_timer.values(), base=2) # python3.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9c5bc-8383-4065-b038-b2eb2f5bfb09",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. При всех ли значениях размера батча достигается 100% точность на тестовой/обучающей выборках?\n",
    "1. Что будет, если понизить коэффициент скорости градиентого спуска `lr`? Добейтесь, чтобы все графики выглядели \"хорошо\".\n",
    "1. Запишите правило: чем больше размер батча, тем скорость обучения выше/ниже?\n",
    "1. Что наблюдается со временем обучения (секунды, потреченные на обучение) при повышении размера батча?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df706dc-0790-4cbc-8952-72756fdc7bef",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1df5f-3ad1-4a36-be13-25ddb3bec580",
   "metadata": {},
   "source": [
    "### Исследование функций активации скрытого слоя\n",
    "\n",
    "Переберите функции активаций.\n",
    "Постройте графики зависимостей:\n",
    "\n",
    "* функции потерь от номера эпохи;\n",
    "* точность классификации от номера эпохи для обучающей и тестовой выборок;\n",
    "* время обучения модели от количества нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79e542-f955-4f07-a5b2-db294c318989",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(3*6, 4))\n",
    "activation_timer = {}\n",
    "for p in params['activation']:\n",
    "    model = create_model(units=50, lr=1.5, activation=p)\n",
    "    t = timeit.default_timer()\n",
    "    history = model.fit(*d2.train, validation_data=d2.test, batch_size=16, epochs=100, verbose=0)\n",
    "    ???\n",
    "    \n",
    "plt.figure()\n",
    "plt.ylabel('Время, секунды')\n",
    "plt.title('Производительность')\n",
    "plt.bar(activation_timer.keys(), activation_timer.values()) # python3.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd3793-d796-4476-9592-370b90f5868f",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. При какой(их) функции активации скрытого слоя достигается максимальная точность?\n",
    "1. При какой(их) функции активации скрытого слоя максимальная точности достигается быстрее всего?\n",
    "1. Как влияет функция активации на время обучения (секунды, потреченные на обучение)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1113a00-e8a9-4833-841e-8290355c487a",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38f584-1e4b-481f-a995-71cfe6aa19c1",
   "metadata": {},
   "source": [
    "## Часть №3. Применение Dropout\n",
    "\n",
    "Применение dropout для решения проблем переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35326d3c-05ca-4a59-9303-2d4ddfc16fcf",
   "metadata": {},
   "source": [
    "### Составление датасета\n",
    "\n",
    "Подготовим датасет, который отвечает следующим критериями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1561124-d9a9-4044-98c0-088f09081f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sk.datasets.make_classification(n_samples=300, n_classes=5, n_features=4, n_informative=3, n_redundant=0, n_clusters_per_class=1, random_state=1)\n",
    "d3 = MyDataset(*sk.model_selection.train_test_split(x, y, test_size=0.3))\n",
    "\n",
    "print(d3.x_train.shape, d3.x_train.dtype, d3.y_train.shape, d3.y_train.dtype, d3.x_test.shape)\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(2*6, 4))\n",
    "for x, y, ax in [(*d3.train, ax0), (*d3.test, ax1)]:\n",
    "    ax.scatter(x[:, 0], x[:, 1], c=y)\n",
    "    ax.set(xlabel='X0', ylabel='X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fdca1-e9d6-49a0-9d9c-3087bf2d6236",
   "metadata": {},
   "source": [
    "### Составление модели\n",
    "\n",
    "В нейросетевой модели сделаем один скрытый слой с большим количеством нейронов.\n",
    "Постройте графики зависимостей:\n",
    "\n",
    "* функции потерь от номера эпохи для обучающей и тестовой выборок;\n",
    "* точность классификации от номера эпохи для обучающей и тестовой выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdb96d-836a-4303-b16a-6cea5710fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def create_model3():\n",
    "    inputs = keras.layers.Input(shape=(d3.x_train.shape[-1],), name=\"inputs\")\n",
    "    x = MyLayer(100, seed=12, name='linear_1')(inputs)\n",
    "    x = keras.layers.Activation('relu', name='activation_1')(x)\n",
    "    x = MyLayer(len(set(d3.y_train)), seed=47, name='linear_end')(x)\n",
    "    outputs = keras.layers.Activation('softmax', name='activation_end')(x)\n",
    "    model = MyModel(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=GradDescent(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model3 = create_model3()\n",
    "model3.summary()\n",
    "history3 = model3.fit(*d3.train, epochs=500, validation_data=d3.test, verbose=0)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(6*2, 4))\n",
    "ax0.plot(history3.history['loss'], label='train')\n",
    "ax0.plot(history3.history['val_loss'], label='test')\n",
    "ax0.set(xlabel=???, ???)\n",
    "ax0.legend()\n",
    "ax1.plot(???, label='train')\n",
    "???\n",
    "\n",
    "result = model3.evaluate(*d3.test, verbose=0)\n",
    "print(dict(zip(model3.metrics_names, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c434b32-b77e-4712-b01b-84a4ec157c39",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1. Насколько сильно отличается точность на обучающей и тестовой выборках?\n",
    "1. Наблюдается ли переобучение?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289431c8-cefb-4d46-980a-198165c8b986",
   "metadata": {},
   "source": [
    "Ответы:\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9c03-f308-4dca-9150-fede6689496b",
   "metadata": {},
   "source": [
    "### Разработка слоя Dropout\n",
    "\n",
    "Реализуйте слой Dropout.\n",
    "Так как этот слой применяется только при обучении модели, необходимо анализировать параметр `training`, чтобы отлючать функциональность слоя при обычном запуске модели.\n",
    "Все вычисления в Tensorflow проводятся в отложенном режиме.\n",
    "Это означает, что результат вычислений будет известен не в точке написания формулы, а где-нибудь в другом месте.\n",
    "\n",
    "При разработке слоя используйте функции:\n",
    "\n",
    "* `tf.random.uniform` - возвращает вектор случайных чисел из заданного интервала (обязательно передайте параметр `shape` и `seed`);\n",
    "* возможно потребуется функция `tf.cast`, чтобы поменять тип некоторых операндов;\n",
    "* примитивные операции (умножение, сложение и т.д.) можно делать так, как есть;\n",
    "* над операндами можно проводить операции сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c92c2-2987-412d-845d-8ac0317363aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, seed, **kwargs):\n",
    "        kwargs['trainable'] = False\n",
    "        super(MyDropout, self).__init__(**kwargs)\n",
    "        self.rate_ = rate\n",
    "        self.scale_ = 1 / (1 - rate)\n",
    "        self.seed_ = seed\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            return ??? tf.cast, tf.random.uniform\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {**super(MyDropout, self).get_config(), 'rate': self.rate_, 'seed': self.seed_}\n",
    "\n",
    "# Тестирование\n",
    "\n",
    "drop_test = MyDropout(0.25, 49)\n",
    "t = drop_test(tf.ones([300, 10]), training=True)\n",
    "\n",
    "assert 0.2 < len(t[t == 0]) / 3000 < 0.3, 'Вероятность dropout вне исследуемого диапазона'\n",
    "assert tf.math.reduce_all(t[t != 0] == 4/3), 'Масштабирование переменных не корректно'\n",
    "try:\n",
    "    drop_test(keras.Input(shape=(10,)), training=True)\n",
    "except:\n",
    "    assert False, 'Не поддерживается частичная форма тензора'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770bafc-ecde-4cf8-bfdd-bd1b3aa8d912",
   "metadata": {},
   "source": [
    "### Составление модели с Dropout\n",
    "\n",
    "В модель добавить слой `MyDropout`.\n",
    "Необходимо выяснить закономерность параметра `rate`.\n",
    "Постройте графики зависимости функции потерь и точности классификации для обучающей и тестовой выборок.\n",
    "Графики сгруппируйте так: отдельно обучающая выборка, отдельно тестовая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288e3c8-9a99-4daf-bd7c-31fd43849e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def create_model32(rate):\n",
    "    inputs = keras.layers.Input(shape=(d3.x_train.shape[-1],), name=\"inputs\")\n",
    "    x = ???\n",
    "    outputs = keras.layers.Activation('softmax', name='activation_end')(x)\n",
    "    model = MyModel(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=GradDescent(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "fig, ((ax0, ax2), (ax1, ax3)) = plt.subplots(2, 2, figsize=(2*6, 2*4), constrained_layout=True)\n",
    "for p in [0, 0.01, 0.02, 0.05, 0.1, 0.25, 0.6, 0.9]:\n",
    "    model = create_model32(rate=p)\n",
    "    history = model.fit(*d3.train, validation_data=d3.test, epochs=500, verbose=0)\n",
    "    print('rate', p, dict(zip(model.metrics_names, model.evaluate(*d3.test, verbose=0))))\n",
    "    ax0.plot(history.history[???], label=p)\n",
    "    ax0.set(xlabel='Номер эпохи', ylabel='Функция штрафа', title='Loss')\n",
    "    ax0.legend()\n",
    "    ax1.plot(???)\n",
    "    ax1.set(xlabel='Номер эпохи', ylabel='Функция штрафа', title='Test Loss')\n",
    "    ax1.legend()\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69f86d-ec03-4a9a-b834-98300bedbc33",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "\n",
    "1. Как меняется точность классификации на обучающей выборке при повышении значения параметра `rate`?\n",
    "1. Как меняется точность классификации на тестовой выборке при повышении значения параметра `rate`?\n",
    "1. Введение Dropout как-то решает проблему с переобучением?\n",
    "1. Удалось ли повысить точность классификации с помощью введения слоя Dropout? Поэкспериментируйте и получите параметр `rate`, при котором получается наибольшая точность классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2401fec-54f9-454a-9613-4292e5c1f410",
   "metadata": {},
   "source": [
    "**Ответы:**\n",
    "\n",
    "1. ...\n",
    "1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
